[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Logistic Regression and Support Vector Machine classifier",
    "section": "",
    "text": "1. Logistic Regression\nThe logistic function (which is more commonly called sigmoid function) basically maps an input to an output of values between 0 and 1. And it is defined as follows: \\[y(z) = \\frac{1}{1+ exp(-z)}\\]\nWe can visualize it as follows:\nFirst define the logistic function:\n\nimport numpy as np\ndef sigmoid(input):\n    return 1.0 / (1 + np.exp(-input))\n\n\n# Input variables from -8 to 8, and the output correspondingly:\nz = np.linspace(-8, 8, 1000)\ny = sigmoid(z)\nimport matplotlib.pyplot as plt\nplt.plot(z, y)\nplt.axhline(y=0, ls='dotted', color='k')\nplt.axhline(y=0.5, ls='dotted', color='k')\nplt.axhline(y=1, ls='dotted', color='k')\nplt.yticks([0.0, 0.25, 0.5, 0.75, 1.0])\nplt.xlabel('z')\nplt.ylabel('y(z)')\nplt.show()\n\n\n\n\n\n\n\n\nIn the S-shaped curve, all inputs are transformed into the range from 0 to 1. For positive inputs, a greater value results in an output closer to 1; for negative inputs, a smaller value generates an output closer to 0; when the input is 0, the output is the midpoint 0.5.\nLogistic regression, by default, is limited to two-class classification problems. It cannot be used for classification tasks that have more than two class labels, so-called multi-class classification. Instead, it requires modification to support multi-class classification problems.\nThe multinomial logistic regression (often just called ‘multinomial regression’) algorithm is an extension to the logistic regression model that involves changing the loss function to cross-entropy loss and predict probability distribution to a multinomial probability distribution to natively support multi-class classification problems.\n\n\n2. Support Vector Machine\nSupport Vector Machines (or SVM) seek a line that best separates two classes. Those data instances that are closest to the line that best separates the classes are called support vectors and influence where the line is placed. SVM has been extended to support multiple classes.Of particular importance is the use of different kernel functions via the kernel parameter.\nKernels can be used that transform the input space into higher dimensions such as a Polynomial Kernel and a Radial Kernel. This is called the Kernel Trick. It is desirable to use more complex kernels as it allows lines to separate the classes that are curved or even more complex. This in turn can lead to more accurate classifiers.\nStratified k-fold cross-validation is a technique used to evaluate the performance of machine learning models, particularly in classification tasks, where the target class distribution may be imbalanced. In this method, the dataset is divided into ‘k’ equally sized folds, ensuring that each fold maintains a similar distribution of class examples as the entire dataset. This stratification process helps to reduce the bias and variance in model performance estimation by preventing a skewed distribution of classes in the train and test sets.\nDuring the cross-validation process, the model is trained on ‘k-1’ folds and tested on the remaining fold, iterating this process ‘k’ times. Each iteration uses a different fold for testing, and the average performance metric (e.g., accuracy) is calculated over all iterations.\nHere’s a code example using the scikit-learn library:\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport numpy as np\n\n# Load the dataset\niris = load_iris()\nX, y = iris.data, iris.target\n\n# Initialize stratified k-fold cross-validation\nskf = StratifiedKFold(n_splits=5)  # Set the number of folds to 5\n\n# Initialize the logistic regression model\nmodel = LogisticRegression(solver='lbfgs', max_iter=1000)\n\n# Perform stratified k-fold cross-validation\naccuracy_scores = []\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    model.fit(X_train, y_train)\n    accuracy = model.score(X_test, y_test)\n    accuracy_scores.append(accuracy)\n\n# Calculate the mean accuracy\nmean_accuracy = np.mean(accuracy_scores)\nprint(\"Mean accuracy:\", mean_accuracy)\n\nMean accuracy: 0.9733333333333334\n\n\nIn this example, we use the Iris dataset and a logistic regression model to demonstrate stratified k-fold cross-validation with 5 folds. The performance of the model is evaluated using accuracy as the performance metric, and the mean accuracy is reported.\nStratified cross-validation is particularly useful when dealing with imbalanced datasets, where some classes have significantly fewer examples compared to others. In such cases, using standard cross-validation might lead to situations where one or more folds contain very few or even none of the underrepresented class instances. This could result in an inaccurate and biased performance estimation of the model, as the model is not adequately tested on all classes.\nFor balanced datasets, where class distributions are roughly equal, stratified cross-validation may not provide significant benefits over standard cross-validation. However, it is still a good practice to use stratified cross-validation as a default approach, as it generally leads to more stable and reliable performance estimates.\nEvaluation Metrics: Provide mean weighted F1 scores and a confusion matrix to evaluate and compare the performance of the classification models.\nHere is an example code how to compute mean weighted F1 score in k-fold cross-validation setting:\n\nf1_scores = []\n\n# Perform k-fold stratified cross-validation\nfor train_index, test_index in cv.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    # Necessary code to compute the predictions using your classifier..\n    #...\n    # y_pred = ...\n\n    # Compute the weighted-average F1-score for this fold\n    fold_f1_score = f1_score(y_test, y_pred, average='weighted')\n    f1_scores.append(fold_f1_score)\n\n# Calculate the mean F1-score across all folds\nmean_weighted_f1_score = np.mean(f1_scores)\nprint(\"Mean weighted-average F1-score across\", k, \"folds:\", mean_weighted_f1_score)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-6-f45df9fe7738&gt; in &lt;cell line: 4&gt;()\n      2 \n      3 # Perform k-fold stratified cross-validation\n----&gt; 4 for train_index, test_index in cv.split(X, y):\n      5     X_train, X_test = X[train_index], X[test_index]\n      6     y_train, y_test = y[train_index], y[test_index]\n\nNameError: name 'cv' is not defined\n\n\n\n\n\nSubmissions\nAfter training your models with the data in train.csv, use the given test.csv data for your predictions (predicted ‘type’ values); save it as csv file, and upload it to the Kaggle competition. Furthermore, you are supposed to upload your coding in Jupyter notebook to the department’s submission system.\nKaggle competition for logistic regression: https://www.kaggle.com/t/1aad2bcbe8c14b80918ec67ef7041c9b\nKaggle competition for support vector machine: https://www.kaggle.com/t/f4f6aee930674ea3a86aada32d861cfa\nDue Date: 30.04.2023\nNote: The late submission rules previously announced from the piazza apply.\n\n\nAssignment\nIn this assignment, you are given a dataset comprising information about dinosaurs. You will use logistic regression and support vector machine models to predict the type of dinosaur based on the provided information. In this assignment, you may utilize built-in libraries. Employ stratified k-fold cross-validation (CV) with 10 folds for evaluating the classification models. Stratification ensures that each CV fold maintains a similar distribution of class examples as the entire training set. You can design various experiments by selecting some/all information provided in the dataset. Here, we expect the best result you obtained after these experiments and observations. Please explicitly mention your feature selection method in your report while presenting results.\n\n\nYour Work\n\n#Include your codes below by including as many cells as necessary to clearly demonstrate your work\n#Please write your codes in separate sections for data pre-processing, Logistic Regression and SVM models etc.\n\n\n\nUpload data\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv('train.csv')\ndata_clone = data.copy()\n\n\n\nData Pre-Processing\n\nprint(data.columns.tolist())\n\nprint(pd.get_dummies(data[['species']]).shape)                                  #unique bundan hic olmaz----\nprint(pd.get_dummies(data[['taxonomy']]).shape)                                 #looks like unique but we may work on it\nprint(pd.get_dummies(data[['lived_in']]).shape)                                 #good\nprint(pd.get_dummies(data[['diet']]).shape)                                     #good\nprint(pd.get_dummies(data[['length']]).shape)                                   #good\nprint(pd.get_dummies(data[['period']]).shape)                                   #looks like unique but we may work on it\nprint(pd.get_dummies(data[['name']]).shape)                                     #unique bundan da hic olmaz----\n\nprint(pd.get_dummies(data[['type']]).shape)                                     #target\n\n['id', 'name', 'diet', 'period', 'lived_in', 'type', 'length', 'taxonomy', 'named_by', 'species', 'link']\n(246, 224)\n(246, 92)\n(246, 30)\n(246, 4)\n(246, 64)\n(246, 127)\n(246, 246)\n(246, 6)\n\n\n\ndata_clone = data_clone.drop('link', axis=1)\ndata_clone = data_clone.drop('species', axis=1)\ndata_clone = data_clone.drop('name', axis=1)\ndata_clone = data_clone.drop('named_by', axis=1)\ndata_clone = data_clone.drop('id', axis=1)\n\n\nprint(data_clone.isnull().sum())\ndata_clone['length'] = data_clone['length'].str.replace('m','').astype(float)   #length parametresini duzelt\ndata_clone['length']\n\ndiet         0\nperiod       0\nlived_in     0\ntype         0\nlength      13\ntaxonomy     0\ndtype: int64\n\n\n0        NaN\n1       5.15\n2      12.00\n3      21.00\n4      10.00\n       ...  \n241      NaN\n242     3.00\n243     0.80\n244     7.60\n245    12.00\nName: length, Length: 246, dtype: float64\n\n\n\n# length in this part we fill null values with before nonnull value and it is not quite good technic.\nfilled_data = data_clone.groupby('lived_in', group_keys=False).apply(lambda x: x.fillna({'length': x['length'].bfill()}))\n# So i decided to fill null values with mean of nonnull values of lived_in. So group by lived_in and fill length with its mean\n\nlength_filled_data = data_clone.groupby('lived_in' , group_keys=False).apply(lambda x: x.fillna({'length': x['length'].fillna(x['length'].mean())}))\ndata_clone['length'] = length_filled_data['length'].round().astype(int)\n\n# burda ise aynisini bu sefer lived_in verisinin meani ile dolduruyoruz\n# bu kod typelara gore guruplayip null length degerleri bir onceki degerle dolduruyor\n\n\n#period data has to be handled!!\ndata_periodic = data_clone['period'].str.split(' ').str[0] +' ' +  data_clone['period'].str.split(' ').str[1]\n\n\n#time periodic data\ndata_time_zone = data_clone['period'].str.split(' ').str[2].str.split('-').str[0] + ' ' +data_clone['period'].str.split(' ').str[2].str.split('-').str[1]\n\ndroped_null_time_zone = data_clone['period'].str.split(' ').str[2].str.split('-')\n\ndata_time_zone1 = round((droped_null_time_zone.str[0].astype(float).fillna(0)).astype(int), -1)\ndata_time_zone2 =  round((droped_null_time_zone.str[1].astype(float).fillna(0)).astype(int), -1)\n\ndata_time_zone[data_time_zone2 != 0] = (data_time_zone1 + data_time_zone2)//2\ndata_time_zone[data_time_zone2 == 0] = data_time_zone1\n\ndata_clone['period'] = data_time_zone\n\ngrouped_mean = (((data_clone.groupby(data_periodic)['period'].mean()).astype(int)/5).round() *5).astype(int)\n\nmask = (data_clone['period'] == 0)\nindices = data_clone.loc[mask].index\ndata_clone.loc[indices, 'period'] = grouped_mean[data.loc[indices, 'period']].values\n\ndata_clone['period_name'] = data_periodic\n\ndata_clone\n#period is good now\n\n\n  \n    \n      \n\n\n\n\n\n\ndiet\nperiod\nlived_in\ntype\nlength\ntaxonomy\nperiod_name\n\n\n\n\n0\ncarnivorous\n215\nUnited Kingdom\nsmall theropod\n7\nDinosauria\nLate Triassic\n\n\n1\nherbivorous/omnivorous\n215\nArgentina\nsauropod\n5\nDinosauria Saurischia Sauropodomorpha Prosauro...\nLate Triassic\n\n\n2\nherbivorous\n75\nChina\neuornithopod\n12\nDinosauria Ornithischia Genasauria Cerapoda Or...\nLate Cretaceous\n\n\n3\nherbivorous\n65\nUSA\nsauropod\n21\nDinosauria Saurischia Sauropodomorpha Sauropod...\nLate Cretaceous\n\n\n4\nherbivorous\n125\nUnited Kingdom\neuornithopod\n10\nDinosauria Ornithischia Genasauria Cerapoda Or...\nEarly Cretaceous\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n241\nherbivorous\n110\nMongolia\narmoured dinosaur\n4\nDinosauria Ornithischia Genasauria Thyreophora...\nEarly Cretaceous\n\n\n242\nherbivorous\n110\nAustralia\neuornithopod\n3\nDinosauria Ornithischia Genasauria Cerapoda Or...\nEarly Cretaceous\n\n\n243\ncarnivorous\n120\nChina\nsmall theropod\n1\nDinosauria Saurischia Theropoda Neotheropoda T...\nEarly Cretaceous\n\n\n244\ncarnivorous\n70\nArgentina\nlarge theropod\n8\nDinosauria Saurischia Theropoda Neotheropoda C...\nLate Cretaceous\n\n\n245\nomnivorous\n225\nSouth Africa\nsauropod\n12\nDinosauria Saurischia Sauropodomorpha Prosauro...\nLate Triassic\n\n\n\n\n246 rows × 7 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# taxonomy has to be handled\ntaxonomy_data = (data_clone['taxonomy'].str.split(' ').str[0] + \" \" +data_clone['taxonomy'].str.split(' ').str[1] + \" \" +data_clone['taxonomy'].str.split(' ').str[2]+ \" \" +data_clone['taxonomy'].str.split(' ').str[3]).fillna(method='bfill')\n\ndata_clone['taxonomy'] = taxonomy_data\n\n# sadece taxonomy verisinin ilk 4 kismini kullanacam ilk 4 kismi olmayan verileri bir onceki veri ile doldurudum\n\n\nprint(data_clone.columns.to_list())\nprint(data.columns.to_list())\n\n['diet', 'period', 'lived_in', 'type', 'length', 'taxonomy', 'period_name']\n['id', 'name', 'diet', 'period', 'lived_in', 'type', 'length', 'taxonomy', 'named_by', 'species', 'link']\n\n\n\nprint('taxonomy', pd.get_dummies(data[['taxonomy']]).shape , pd.get_dummies(data_clone[['taxonomy']]).shape, sep=' ---&gt; ')\nprint('lived_in', pd.get_dummies(data[['lived_in']]).shape,(pd.get_dummies(data_clone[['lived_in']]).shape) , sep=' ---&gt; ')\nprint('diet', pd.get_dummies(data[['diet']]).shape,(pd.get_dummies(data_clone[['diet']]).shape)  , sep=' ---&gt; ')\nprint('length', pd.get_dummies(data[['length']]).shape,(pd.get_dummies(data_clone[['length']]).shape)  , sep=' ---&gt; ')\nprint('period', pd.get_dummies(data[['period']]).shape,(pd.get_dummies(data_clone[['period']]).shape)  , sep=' ---&gt; ')\n\nprint('new feature period_name ', pd.get_dummies(data_clone[['period_name']]).shape  , sep='---&gt; ')\n\nprint('target feature type ', pd.get_dummies(data_clone[['type']]).shape  , sep='---&gt; ')\n\ntaxonomy ---&gt; (246, 92) ---&gt; (246, 6)\nlived_in ---&gt; (246, 30) ---&gt; (246, 30)\ndiet ---&gt; (246, 4) ---&gt; (246, 4)\nlength ---&gt; (246, 64) ---&gt; (246, 1)\nperiod ---&gt; (246, 127) ---&gt; (246, 34)\nnew feature period_name ---&gt; (246, 6)\ntarget feature type ---&gt; (246, 6)\n\n\n\n\nTest Data Preprocesing\n\ntest_data = pd.read_csv('test.csv')\n\nfeatures = ['id','diet', 'period', 'lived_in', 'length', 'taxonomy']\ntarget = 'type'\n\ntest_copy = test_data[features]\n\n#fix the taxonomy parameter of test data\ntaxonomy_data_test = (test_copy['taxonomy'].str.split(' ').str[0] + \" \" +test_copy['taxonomy'].str.split(' ').str[1] + \" \" +test_copy['taxonomy'].str.split(' ').str[2]+ \" \" +test_copy['taxonomy'].str.split(' ').str[3]).fillna(method='bfill')\ntest_copy['taxonomy'] = taxonomy_data_test\n\n#fix the lived_in parameter of test data\ntest_copy['lived_in'].fillna(test_copy['period'], inplace=True)\n\n#fix the period parameter of test data\ndata_periodic_test = test_copy['period'].str.split(' ').str[0] +' ' +  test_copy['period'].str.split(' ').str[1]\ntest_copy['period_name'] = data_periodic_test\ntest_copy['period_name'] = test_copy.groupby('taxonomy')['period_name'].transform(lambda x: x.fillna(x.mode()[0]))\n\ndata_time_zone_test = test_copy['period'].str.split(' ').str[2].str.split('-').str[0] + ' ' +test_copy['period'].str.split(' ').str[2].str.split('-').str[1]\ndroped_null_time_zone = test_copy['period'].str.split(' ').str[2].str.split('-')\ndata_time_zonex = round((droped_null_time_zone.str[0].astype(float).fillna(0)).astype(int), -1)\ndata_time_zoney =  round((droped_null_time_zone.str[1].astype(float).fillna(0)).astype(int), -1)\ndata_time_zone_test[data_time_zoney != 0] = (data_time_zonex + data_time_zoney)//2\ndata_time_zone_test[data_time_zoney == 0] = data_time_zonex\ntest_copy['period'] = data_time_zone_test\ngrouped_mean_test = (((test_copy.groupby(data_periodic_test)['period'].mean()).astype(int)/5).round() *5).astype(int)\nmask = (test_copy['period'] == 0)\nindices = test_copy.loc[mask].index\ntest_copy.loc[indices, 'period'] = grouped_mean_test[test_copy.loc[indices, 'period_name']].values\ntest_copy['period_name'] = data_periodic_test\n\n#fix the length parameter of test data\ntest_copy['length'] = test_copy['length'].str.replace('m','').astype(float)\n\nfilled_test_copy =  test_copy.groupby('taxonomy' , group_keys=False).apply(lambda x: x.fillna({'length': x['length'].fillna(x['length'].mean())}))\ntest_copy['length'] = filled_test_copy['length'].round()\n\n\n\nLogistic Regression\n\n#test of features that is true combination for Logistic Regression\nfeatures = ['diet', 'period', 'lived_in', 'length', 'taxonomy', 'period_name']\ntarget = 'type'\n\nX = data_clone[features]\ny = data_clone[target]\n\nbiggest_subset = 0\nbiggest_accuracy = 0\nfor n_features in range(len(features), 0, -1):\n    for feature_comb in itertools.combinations(features, n_features):\n        X_comb = pd.get_dummies(X[list(feature_comb)])\n        #stratified k-fold cross-validation (CV) with 10 folds\n        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\n        model = LogisticRegression()\n        acc_list = []\n        for train_idx, test_idx in skf.split(X_comb, y):\n            X_train, y_train = X_comb.iloc[train_idx], y.iloc[train_idx]\n            X_test, y_test = X_comb.iloc[test_idx], y.iloc[test_idx]\n            model.fit(X_train, y_train)\n            accuracy = model.score(X_test, y_test)\n            acc_list.append(accuracy)\n        avg_accuracy = sum(acc_list) / len(acc_list)\n        if biggest_accuracy &lt; avg_accuracy:\n            biggest_accuracy = avg_accuracy\n            biggest_subset = feature_comb\n        #print(f\"Features: {list(feature_comb)}, Mean Accuracy: {sum(acc_list)/len(acc_list):.3f}\")\nprint(f\"Features: {biggest_subset} - Average Accuracy: {biggest_accuracy}\")\n\nFeatures: ('diet', 'period', 'lived_in', 'length', 'taxonomy') - Average Accuracy: 0.866\n\n\n\ntest_copy[['diet', 'period', 'lived_in', 'length', 'taxonomy']].isnull().sum()\n\ntest_clone_for_lr = test_copy[['id','diet', 'period', 'lived_in', 'length', 'taxonomy']]\n\nfeatures = ['diet', 'period', 'lived_in', 'length', 'taxonomy']\ntarget = 'type'\n\nX = data_clone[features]\ny = data_clone[target]\n\nX_test = pd.get_dummies(test_clone_for_lr[features])\nX_comb = pd.get_dummies(X)\n\ntest_copy_encoded = pd.get_dummies(test_clone_for_lr[features])\nX_test = test_copy_encoded.reindex(columns = X_comb.columns, fill_value=0)\n\nmodel = LogisticRegression()\nmodel.fit(X_comb.values, y.values)\n\naccuracy = model.score(X_comb, y)\nprint(f\"Accuracy for trained data: {accuracy:.3f}\")\n\ny_pred = model.predict(X_test)\n\ntest_clone_for_lr[target] = y_pred\n\ntest_clone_for_lr[['id','diet', 'period', 'lived_in', 'length', 'taxonomy', 'type']].info()\n\nAccuracy for trained data: 0.943\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 62 entries, 0 to 61\nData columns (total 7 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   id        62 non-null     int64  \n 1   diet      62 non-null     object \n 2   period    62 non-null     object \n 3   lived_in  62 non-null     object \n 4   length    62 non-null     float64\n 5   taxonomy  62 non-null     object \n 6   type      62 non-null     object \ndtypes: float64(1), int64(1), object(5)\nmemory usage: 3.5+ KB\n\n\n\nlr_predicts = pd.DataFrame(test_clone_for_lr[['id', 'type']])\nlr_predicts.columns = ['id', 'type']\n\nlr_predicts.to_csv('prediction_lr.csv', index=False)\n\n\n\nSupport Vector Machine\n\n#test of features that is true combination for Support Vector Machine\n\nfeatures = ['diet', 'period', 'lived_in', 'length', 'taxonomy', 'period_name']\ntarget = 'type'\n\nX = data_clone[features]\ny = data_clone[target]\n\nbiggest_subset = itertools.combinations(features, 1)\nbiggest_accuracy = 0\nfor i in range(len(features), 0, -1):\n    for subset in itertools.combinations(features, i):\n        X_sub = pd.get_dummies(X[list(subset)])\n        #stratified k-fold cross-validation (CV) with 10 folds\n        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\n        model = SVC(kernel='rbf')\n        accuracy_list = []\n        for train_idx, test_idx in skf.split(X_sub, y):\n            X_train, y_train = X_sub.iloc[train_idx], y.iloc[train_idx]\n            X_test, y_test = X_sub.iloc[test_idx], y.iloc[test_idx]\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n            accuracy = accuracy_score(y_test, y_pred)\n            accuracy_list.append(accuracy)\n        avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n        if biggest_accuracy &lt; avg_accuracy:\n          biggest_accuracy = avg_accuracy\n          biggest_subset = subset\n        #print(f\"Features: {subset} - Average Accuracy: {avg_accuracy}\")\n\n\nprint(f\"Features: {biggest_subset} - Average Accuracy: {biggest_accuracy}\")\n\nFeatures: ('diet', 'lived_in', 'taxonomy') - Average Accuracy: 0.7679999999999999\n\n\n\ntest_copy[['diet', 'period', 'lived_in', 'length', 'taxonomy']].isnull().sum()\n\ntest_clone_for_svm = test_copy[['id', 'diet', 'period', 'lived_in', 'length', 'taxonomy']]\n\nfeatures = ['diet', 'lived_in',  'taxonomy']\ntarget = 'type'\n\nX = data_clone[features]\ny = data_clone[target]\n\nX_test = pd.get_dummies(test_clone_for_svm[features])\nX_comb = pd.get_dummies(X)\n\ntest_copy_encoded = pd.get_dummies(test_clone_for_svm[features])\nX_test = test_copy_encoded.reindex(columns = X_comb.columns, fill_value=0)\n\nmodel = SVC(kernel='rbf')\nmodel.fit(X_comb.values, y.values)\n\naccuracy = model.score(X_comb, y)\nprint(f\"Accuracy for trained data: {accuracy:.3f}\")\n\n\ny_pred = model.predict(X_test)\n\ntest_clone_for_svm[target] = y_pred\n\ntest_clone_for_svm[['id', 'diet', 'lived_in',  'taxonomy', 'type']]\n\nAccuracy for trained data: 0.825\n\n\n\n  \n    \n      \n\n\n\n\n\n\nid\ndiet\nlived_in\ntaxonomy\ntype\n\n\n\n\n0\n1\nherbivorous\nRomania\nDinosauria Saurischia Sauropodomorpha Sauropoda\nsauropod\n\n\n1\n2\nherbivorous\nUSA\nDinosauria Saurischia Sauropodomorpha Sauropoda\nsauropod\n\n\n2\n3\ncarnivorous\nUSA\nDinosauria Saurischia Theropoda Neotheropoda\nlarge theropod\n\n\n3\n4\ncarnivorous\nUSA\nDinosauria Saurischia Theropoda Neotheropoda\nlarge theropod\n\n\n4\n5\nomnivorous\nUSA\nDinosauria Saurischia Theropoda Neotheropoda\nlarge theropod\n\n\n...\n...\n...\n...\n...\n...\n\n\n57\n58\nherbivorous\nChina\nDinosauria Saurischia Sauropodomorpha Sauropoda\nsauropod\n\n\n58\n59\nherbivorous\nMongolia\nDinosauria Ornithischia Genasauria Thyreophora\narmoured dinosaur\n\n\n59\n60\ncarnivorous\nMorocco\nDinosauria Saurischia Theropoda Neotheropoda\nlarge theropod\n\n\n60\n61\nherbivorous\nCanada\nDinosauria Ornithischia Genasauria Cerapoda\nceratopsian\n\n\n61\n62\nherbivorous\nCanada\nDinosauria Ornithischia Genasauria Cerapoda\nceratopsian\n\n\n\n\n62 rows × 5 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsvm_predicts = pd.DataFrame(test_clone_for_svm[['id', 'type']])\nsvm_predicts.columns = ['id', 'type']\n\nsvm_predicts.to_csv('prediction_svm.csv', index=False)",
    "crumbs": [
      "Logistic Regression and Support Vector Machine classifier"
    ]
  }
]